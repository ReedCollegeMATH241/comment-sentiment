{"comment_text": "And sorry for spelling this out for you, but maybe it is you who is &quot;accusing of making elementary mistakes without actually understanding what he&#x27;s doing.&quot;<p>Here is why:<p>1)\n&gt; Your polynomial, since it doesn&#x27;t get it exactly right at 1&#x2F;4 &amp; 1&#x2F;2, will have an enormous relative error when you use it for computing logs of numbers very close to 1.<p>In case you missed, Mathematica&#x27;s (not mine) polynomial gives\na maximum 0.00334047 relative error. And the points with 0.00334047 error are actually edge points (which does <i>not</i> include 1).<p>The important point is, it is <i>not</i> enormous. In fact, his polynomial gives <i>10 times</i> larger error at certain points, so it is he who has an enormous error. And Mathematica&#x27;s polynomial is continuous, so even when you slightly get outside of the intended range, you&#x27;re still getting an error of the order 10^-3.<p>2)\n&gt; Your (2,2) rational approximant will be much more expensive to compute than the polynomial and won&#x27;t yield the speed gains Goldberg is looking for.<p>Again, since you apparently missed it, the (2,2) approximant I gave is for [0.25,0.5] interval with t=1&#x2F;4, so yes, bad things can and will happen if you use that polynomial outside of that range (same things happens with his polynomials too, obviously). Instead, what one should do is to move that 1&#x2F;4 part into the exponent and stick to [0.25,0.5].<p>(About the speed vs accuracy trade-off, I already pointed that out in my original post by saying &quot;if the division isn&#x27;t too expensive&quot;; I&#x27;m not sure why you are emphasizing it again here.)<p>Going back to your statement, when compute logs very close to 1, you subtract 2 from the exponent and instead compute between [0.25,0.5] using that polynomial.<p>3.\n&gt; &quot;Imposing that errors should vanish at certain points&quot; is not a mistake, it&#x27;s a necessity if you&#x27;re implementing log by argument reduction plus local approximation and you care about relative error in the resulting approx_log function, and you are getting better numbers than Goldberg by solving the wrong problem.<p>&quot;argument reduction plus local approximation&quot; means range reduction to [t,2t) for log function and this has nothing to do with the behavior of approximation polynomial at the edges; you may very well have non-zero errors at t and 2t and there is nothing wrong or alarming about it (see Handbook of Floating Point Arithmetic, Chapter 11, for example).\nThe point is, the max rel error should be kept small, which Mathematica does, 10 times better.<p>Such an artificial condition is necessary only when the relative error function becomes discontinuous and starts making sudden jumps (see Figs 6-7 from the top in the write up.) He is trying to remedy a problem which appeared due to his choice of t, at the cost of losing degrees of freedom in the polynomial (see also the comments here <a href=\"http:&#x2F;&#x2F;reference.wolfram.com&#x2F;language&#x2F;FunctionApproximations&#x2F;tutorial&#x2F;FunctionApproximations.html\" rel=\"nofollow\">http:&#x2F;&#x2F;reference.wolfram.com&#x2F;language&#x2F;FunctionApproximations...</a> about when the function becomes zero in the interval, in which case you need to deal with these points by making them special --note from the example that this doesn&#x27;t have to mean you lose 2 degrees of freedom to avoid a single point).<p>So, in short, I <i>am</i> solving the right problem.<p>4. Which brings me to the other unspelled problem in some of his polynomials; the derivative is very bumpy and discontinuous, which spells out disaster in general math applications.\n(Clearly, the derivative of his function will have enormous minimax (L^\\infty) and RMS (L^2) errors, unlike Mathematica&#x27;s polynomial.)<p>Summary: 1) Mathematica&#x27;s polynomial gives ~10 times smaller maximum error than the write up within the range 2) Mathematica&#x27;s polynomial gives 10x smaller RMS error within the range 3) Mathematica&#x27;s polynomial is smooth. (I hope by now, you are clear that Mathematica&#x27;s polynomial nowhere gives any enormous error and quite the oppoisite, the error is <i>much</i> lower) 4) If you use approximant polynomials only within their intended range and move the remaining factors into the exponent, everything works out. \n5) My pointing out a bad mathematical practice wasn&#x27;t personal, so please don&#x27;t steer the discussion into that direction. I understand that you respect him which may motivate this, my credentials in maths aren&#x27;t shabby at all (I have more peer-reviewed research papers than him, and in higher impact journals etc etc) (and if Mathematica says one thing and a human says another, in most cases Mathematica is correct) but  credentials don&#x27;t matter in front of solid results. In fact,  accepting a scientific result (partly-)based on credentials is considered to be toxic in science. Which brings me back to my original point. I stand by what I said earlier: I don&#x27;t recommend using the polynomials in the write up.", "author": "tagrun", "title": null, "url": null, "timestamp": 1431279698, "created_at": "2015-05-10T17:41:38.000Z", "comment_id": "9520651", "parent_id": 9519477, "story_title": "Fast Approximate Logarithms, Part I: The Basics", "story_id": 9516478, "story_url": "http://www.ebaytechblog.com/2015/05/01/fast-approximate-logarithms-part-i-the-basics/", "points": null}